{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Cone Beam FDK Reconstruction\n",
    "## Enhanced with Parallel Processing using Joblib\n",
    "\n",
    "This optimized version includes:\n",
    "- Parallel processing of projections using joblib\n",
    "- Vectorized operations where possible\n",
    "- Memory-efficient implementations\n",
    "- Progress tracking with tqdm\n",
    "- Better code structure and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 24\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from pydicom import dcmread, dcmwrite\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft, ifft\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "PI = math.pi\n",
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_data(path_list):\n",
    "    \"\"\"\n",
    "    Optimized DICOM data loading with parallel processing\n",
    "    \"\"\"\n",
    "    def read_single_dicom(fname):\n",
    "        try:\n",
    "            raw = dcmread(fname)\n",
    "            return raw.pixel_array, raw.GantryAngle\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {fname}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    # Use parallel processing for DICOM reading\n",
    "    n_jobs = min(8, multiprocessing.cpu_count())  # Limit to avoid I/O bottleneck\n",
    "    results = Parallel(n_jobs=n_jobs, backend='threading')(\n",
    "        delayed(read_single_dicom)(fname) for fname in tqdm(path_list, desc=\"Loading DICOM files\")\n",
    "    )\n",
    "    \n",
    "    # Filter out failed reads\n",
    "    valid_results = [(img, angle) for img, angle in results if img is not None]\n",
    "    \n",
    "    if not valid_results:\n",
    "        raise ValueError(\"No valid DICOM files found\")\n",
    "    \n",
    "    images = np.array([img for img, _ in valid_results])\n",
    "    angles = np.array([angle for _, angle in valid_results])\n",
    "    \n",
    "    return images, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_differential_images(images, angles, threshold=10000):\n",
    "    \"\"\"\n",
    "    Optimized differential image processing\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    processed_images = np.zeros_like(images)\n",
    "    processed_angles = []\n",
    "    \n",
    "    prev_img = np.zeros_like(images[0])\n",
    "    \n",
    "    for idx in tqdm(range(n_images), desc=\"Processing differential images\"):\n",
    "        curr_img = images[idx]\n",
    "        diff_img = curr_img - prev_img\n",
    "        \n",
    "        if idx > 0 and np.max(diff_img) > threshold:\n",
    "            # Use previous valid image and angle\n",
    "            processed_images[idx] = processed_images[idx-1]\n",
    "            processed_angles.append(processed_angles[idx-1])\n",
    "        else:\n",
    "            processed_images[idx] = diff_img\n",
    "            processed_angles.append(angles[idx])\n",
    "            \n",
    "        prev_img = curr_img\n",
    "    \n",
    "    return processed_images, np.array(processed_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DICOM files found: 1135\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "_arc1_pth = r\"E:\\CMC\\pyprojects\\radio_therapy\\dose-3d\\dataset\\epid-1-arc-vmat\"\n",
    "_arc2_pth = r\"E:\\CMC\\pyprojects\\radio_therapy\\dose-3d\\dataset\\2\"\n",
    "\n",
    "# Get file lists\n",
    "_files_1 = [f for f in os.listdir(_arc1_pth) if f.endswith('.dcm')]\n",
    "_files_2 = [f for f in os.listdir(_arc2_pth) if f.endswith('.dcm')]\n",
    "\n",
    "# Create full paths\n",
    "_full_paths_1 = [os.path.join(_arc1_pth, f) for f in _files_1]\n",
    "_full_paths_2 = [os.path.join(_arc2_pth, f) for f in _files_2]\n",
    "_pth = _full_paths_1 + _full_paths_2\n",
    "\n",
    "print(f\"Total DICOM files found: {len(_pth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DICOM files: 100%|██████████| 1135/1135 [00:02<00:00, 380.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICOM loading completed in 6.09 seconds\n",
      "Image shape: (1190, 1190)\n",
      "Raw images shape: (1135, 1190, 1190)\n"
     ]
    }
   ],
   "source": [
    "# Load DICOM data\n",
    "start_time = time.time()\n",
    "raw_images, raw_angles = load_dicom_data(_pth)\n",
    "print(f\"DICOM loading completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Get image properties from first DICOM\n",
    "dcm = dcmread(_pth[0])\n",
    "shape = dcm.Rows, dcm.Columns\n",
    "print(f\"Image shape: {shape}\")\n",
    "print(f\"Raw images shape: {raw_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing differential images: 100%|██████████| 1135/1135 [00:02<00:00, 495.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differential processing completed in 3.71 seconds\n",
      "Final sorted images shape: (1135, 1190, 1190)\n",
      "Angle range: 0.1° to 359.9°\n"
     ]
    }
   ],
   "source": [
    "# Process differential images\n",
    "start_time = time.time()\n",
    "processed_images, processed_angles = process_differential_images(raw_images, raw_angles)\n",
    "print(f\"Differential processing completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Sort images by gantry angle\n",
    "sorted_indices = np.argsort(processed_angles)\n",
    "sorted_images = processed_images[sorted_indices]\n",
    "sorted_angles = processed_angles[sorted_indices]\n",
    "\n",
    "print(f\"Final sorted images shape: {sorted_images.shape}\")\n",
    "print(f\"Angle range: {sorted_angles.min():.1f}° to {sorted_angles.max():.1f}°\")\n",
    "\n",
    "# Free memory\n",
    "del raw_images, processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Filter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shepp_logan(N, d):\n",
    "    \"\"\"\n",
    "    Optimized Shepp-Logan filter using vectorized operations\n",
    "    \"\"\"\n",
    "    k = np.arange(N) - N/2.0\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        fh_SL = -2.0 / (PI * PI * d * d * (4 * k**2 - 1))\n",
    "        fh_SL[np.isinf(fh_SL)] = 0  # Handle inf values\n",
    "        fh_SL[np.isnan(fh_SL)] = 0  # Handle NaN values\n",
    "    return fh_SL\n",
    "\n",
    "def nearest_power_of_2(N):\n",
    "    \"\"\"\n",
    "    Find the nearest power of 2 greater than or equal to N\n",
    "    \"\"\"\n",
    "    a = int(math.log2(N))\n",
    "    return N if 2**a == N else 2**(a + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Reconstruction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_projection_vectorized(projection_beta, SOD, delta_dd):\n",
    "    \"\"\"\n",
    "    Vectorized weighting of projection for cone beam geometry\n",
    "    \"\"\"\n",
    "    Nrows, Ncolumns = projection_beta.shape\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    dd_column = delta_dd * np.arange(-Ncolumns/2 + 0.5, Ncolumns/2 + 0.5)\n",
    "    dd_row = delta_dd * np.arange(-Nrows/2 + 0.5, Nrows/2 + 0.5)\n",
    "    dd_row2D, dd_column2D = np.meshgrid(dd_row, dd_column, indexing='ij')\n",
    "    \n",
    "    # Vectorized weight calculation\n",
    "    weight = SOD / np.sqrt(SOD**2 + dd_row2D**2 + dd_column2D**2)\n",
    "    \n",
    "    return projection_beta * weight\n",
    "\n",
    "def filter_projection_fft(weighted_projection, fh_filter):\n",
    "    \"\"\"\n",
    "    Optimized filtering using FFT with proper padding\n",
    "    \"\"\"\n",
    "    Nrows, Ncolumns = weighted_projection.shape\n",
    "    Nfft = nearest_power_of_2(2 * Ncolumns - 1)\n",
    "    \n",
    "    # Prepare filter in frequency domain\n",
    "    fh_padded = np.zeros(Nfft)\n",
    "    fh_padded[:len(fh_filter)] = fh_filter / 2.0\n",
    "    fh_fft = fft(fh_padded)\n",
    "    \n",
    "    # Pad projection data\n",
    "    projection_padded = np.zeros((Nrows, Nfft))\n",
    "    projection_padded[:, :Ncolumns] = weighted_projection\n",
    "    \n",
    "    # Perform filtering in frequency domain\n",
    "    projection_fft = fft(projection_padded, axis=1)\n",
    "    filtered_fft = projection_fft * fh_fft[np.newaxis, :]\n",
    "    filtered_projection = ifft(filtered_fft, axis=1).real\n",
    "    \n",
    "    return filtered_projection[:, :Ncolumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backproject_single_angle(args):\n",
    "    \"\"\"\n",
    "    Process a single projection angle for backprojection\n",
    "    This function is designed to be called in parallel\n",
    "    \"\"\"\n",
    "    projection_beta, beta_rad, SOD, delta_dd, Nimage, fh_filter, beta_num = args\n",
    "    \n",
    "    # Weight and filter the projection\n",
    "    weighted_projection = weight_projection_vectorized(projection_beta, SOD, delta_dd)\n",
    "    filtered_projection = filter_projection_fft(weighted_projection, fh_filter)\n",
    "    \n",
    "    # Perform backprojection\n",
    "    Nrows, Ncolumns = filtered_projection.shape\n",
    "    MX, MZ = Nimage, int(Nimage * Nrows / Ncolumns)\n",
    "    \n",
    "    # Define reconstruction volume\n",
    "    roi = delta_dd * np.array([-Ncolumns/2.0 + 0.5, Ncolumns/2.0 - 0.5, \n",
    "                               -Nrows/2.0 + 0.5, Nrows/2.0 - 0.5])\n",
    "    \n",
    "    hx = (roi[1] - roi[0]) / (MX - 1)\n",
    "    hy = (roi[3] - roi[2]) / (MZ - 1)\n",
    "    \n",
    "    xrange = roi[0] + hx * np.arange(MX)\n",
    "    yrange = roi[2] + hy * np.arange(MZ)\n",
    "    \n",
    "    XX, YY, ZZ = np.meshgrid(xrange, xrange, yrange, indexing='ij')\n",
    "    \n",
    "    # Backprojection geometry\n",
    "    cos_beta, sin_beta = np.cos(beta_rad), np.sin(beta_rad)\n",
    "    \n",
    "    U = (SOD + XX * sin_beta - YY * cos_beta) / SOD\n",
    "    a = (XX * cos_beta + YY * sin_beta) / U\n",
    "    b = ZZ / U\n",
    "    \n",
    "    # Convert to detector coordinates\n",
    "    xx = np.floor(a / delta_dd).astype(np.int32)\n",
    "    yy = np.floor(b / delta_dd).astype(np.int32)\n",
    "    \n",
    "    u1 = a / delta_dd - xx\n",
    "    u2 = b / delta_dd - yy\n",
    "    \n",
    "    # Adjust indices to start from 0\n",
    "    xx += Ncolumns // 2\n",
    "    yy += Nrows // 2\n",
    "    \n",
    "    # Create mask for valid indices\n",
    "    mask = (xx >= 0) & (xx < Ncolumns - 1) & (yy >= 0) & (yy < Nrows - 1)\n",
    "    \n",
    "    # Initialize backprojection result\n",
    "    temp_rec = np.zeros((MX, MX, MZ))\n",
    "    \n",
    "    if np.any(mask):\n",
    "        # Extract valid coordinates\n",
    "        xx_valid = xx[mask]\n",
    "        yy_valid = yy[mask]\n",
    "        u1_valid = u1[mask]\n",
    "        u2_valid = u2[mask]\n",
    "        U_valid = U[mask]\n",
    "        \n",
    "        # Bilinear interpolation\n",
    "        temp_val = ((1 - u1_valid) * (1 - u2_valid) * filtered_projection[yy_valid, xx_valid] +\n",
    "                   (1 - u1_valid) * u2_valid * filtered_projection[yy_valid + 1, xx_valid] +\n",
    "                   u1_valid * (1 - u2_valid) * filtered_projection[yy_valid, xx_valid + 1] +\n",
    "                   u1_valid * u2_valid * filtered_projection[yy_valid + 1, xx_valid + 1])\n",
    "        \n",
    "        # Apply backprojection weight\n",
    "        temp_val = temp_val / (U_valid**2) * 2 * PI / beta_num\n",
    "        \n",
    "        # Add to reconstruction volume\n",
    "        temp_rec[mask] = temp_val\n",
    "    \n",
    "    return temp_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_fdk_reconstruction(projections, angles, SOD, Nimage, delta_dd, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Parallel FDK reconstruction using joblib\n",
    "    \"\"\"\n",
    "    Ncolumns = projections.shape[2]\n",
    "    Nrows = projections.shape[1]\n",
    "    beta_num = len(angles)\n",
    "    \n",
    "    # Convert angles to radians\n",
    "    beta_rad = angles * PI / 180.0\n",
    "    \n",
    "    # Prepare filter\n",
    "    Nfft = nearest_power_of_2(2 * Ncolumns - 1)\n",
    "    fh_filter = filter_shepp_logan(Nfft, delta_dd)\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = []\n",
    "    for m in range(beta_num):\n",
    "        args = (projections[m, :, :], beta_rad[m], SOD, delta_dd, \n",
    "                Nimage, fh_filter, beta_num)\n",
    "        args_list.append(args)\n",
    "    \n",
    "    # Set number of jobs\n",
    "    if n_jobs == -1:\n",
    "        n_jobs = min(multiprocessing.cpu_count(), beta_num)\n",
    "    \n",
    "    print(f\"Starting parallel reconstruction with {n_jobs} processes...\")\n",
    "    \n",
    "    # Parallel processing with progress bar\n",
    "    with tqdm(total=beta_num, desc=\"Reconstructing\") as pbar:\n",
    "        def update_progress(result):\n",
    "            pbar.update(1)\n",
    "            return result\n",
    "        \n",
    "        # Use joblib for parallel processing\n",
    "        backprojections = Parallel(n_jobs=n_jobs, backend='multiprocessing')(\n",
    "            delayed(backproject_single_angle)(args) for args in args_list\n",
    "        )\n",
    "    \n",
    "    # Sum all backprojections\n",
    "    print(\"Summing backprojections...\")\n",
    "    rec_image = np.sum(backprojections, axis=0)\n",
    "    \n",
    "    return rec_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optimized Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction parameters:\n",
      "  Image size: 100x100\n",
      "  SOD: 1000 mm\n",
      "  SDD: 1499.98304881781 mm\n",
      "  Detector pixel size: 0.172 mm\n",
      "  Virtual detector interval: 0.1147 mm\n",
      "  Number of projections: 1135\n"
     ]
    }
   ],
   "source": [
    "# Reconstruction parameters\n",
    "Nimage = 100  # size of reconstructed image\n",
    "SID = dcm.RTImageSID\n",
    "SAD = dcm.RadiationMachineSAD\n",
    "SOD = SAD  # source to origin distance, in unit mm\n",
    "SDD = SID  # source to center of detector, in unit mm\n",
    "width = 0.172  # size of detector cell, in unit mm\n",
    "delta_dd = width * SOD / SDD  # interval of the virtual detector cell\n",
    "\n",
    "print(f\"Reconstruction parameters:\")\n",
    "print(f\"  Image size: {Nimage}x{Nimage}\")\n",
    "print(f\"  SOD: {SOD} mm\")\n",
    "print(f\"  SDD: {SDD} mm\")\n",
    "print(f\"  Detector pixel size: {width} mm\")\n",
    "print(f\"  Virtual detector interval: {delta_dd:.4f} mm\")\n",
    "print(f\"  Number of projections: {len(sorted_angles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel reconstruction with 4 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing:   0%|          | 0/1135 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Run parallel reconstruction\n",
    "start_time = time.time()\n",
    "\n",
    "# Use fewer jobs to avoid memory issues with large datasets\n",
    "n_jobs = min(4, multiprocessing.cpu_count())  # Limit to avoid memory issues\n",
    "\n",
    "rec_image = parallel_fdk_reconstruction(\n",
    "    sorted_images, sorted_angles, SOD, Nimage, delta_dd, n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "reconstruction_time = time.time() - start_time\n",
    "print(f\"\\nParallel reconstruction completed in {reconstruction_time:.2f} seconds\")\n",
    "print(f\"Reconstructed image shape: {rec_image.shape}\")\n",
    "print(f\"Reconstruction value range: {rec_image.min():.2f} to {rec_image.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reconstruction results\n",
    "NimageZ = Nimage * sorted_images.shape[0] // sorted_images.shape[0]\n",
    "Z_c = int(NimageZ // 2)\n",
    "X_c = int(Nimage // 2)\n",
    "Y_c = int(Nimage // 2)\n",
    "\n",
    "figure, axis = plt.subplots(1, 3, figsize=(15, 5))\n",
    "figure.suptitle('Optimized Parallel FDK Reconstruction Results', fontsize=16)\n",
    "\n",
    "# Sagittal view (X plane)\n",
    "axis[0].imshow(rec_image[X_c, :, :].T, cmap='CMRmap_r')\n",
    "axis[0].set_title(f'Sagittal (X={X_c})')\n",
    "axis[0].axis('off')\n",
    "\n",
    "# Coronal view (Y plane)\n",
    "axis[1].imshow(rec_image[:, Y_c, :].T, cmap='CMRmap_r')\n",
    "axis[1].set_title(f'Coronal (Y={Y_c})')\n",
    "axis[1].axis('off')\n",
    "\n",
    "# Axial view (Z plane)\n",
    "axis[2].imshow(rec_image[:, :, Z_c].T, cmap='CMRmap_r')\n",
    "axis[2].set_title(f'Axial (Z={Z_c})')\n",
    "axis[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Center voxel value: {rec_image[X_c, Y_c, Z_c]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with TPS (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TPS reference dose for comparison (update path as needed)\n",
    "try:\n",
    "    _TPS_pth = r\"E:\\CMC\\pyprojects\\radio_therapy\\dose-3d\\dataset\\3DDose\\RD.23022024.12 x 12.dcm\"\n",
    "    \n",
    "    if os.path.exists(_TPS_pth):\n",
    "        tps_dcm = dcmread(_TPS_pth)\n",
    "        tps_image = tps_dcm.pixel_array\n",
    "        \n",
    "        # Reorient reconstruction for comparison\n",
    "        rec_image_oriented = np.transpose(rec_image, (2, 1, 0))\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('Optimized EPID vs TPS Dose Comparison', fontsize=16)\n",
    "        \n",
    "        # EPID reconstruction (top row)\n",
    "        axs[0, 0].imshow(rec_image_oriented[X_c, :, :].T, cmap='CMRmap_r')\n",
    "        axs[0, 0].set_title('EPID - Sagittal')\n",
    "        axs[0, 0].axis('off')\n",
    "        \n",
    "        axs[0, 1].imshow(rec_image_oriented[:, Y_c, :].T, cmap='CMRmap_r')\n",
    "        axs[0, 1].set_title('EPID - Coronal')\n",
    "        axs[0, 1].axis('off')\n",
    "        \n",
    "        axs[0, 2].imshow(rec_image_oriented[:, :, Z_c].T, cmap='CMRmap_r')\n",
    "        axs[0, 2].set_title('EPID - Axial')\n",
    "        axs[0, 2].axis('off')\n",
    "        \n",
    "        # TPS dose (bottom row)\n",
    "        axs[1, 0].imshow(tps_image[X_c, :, :].T, cmap='CMRmap_r')\n",
    "        axs[1, 0].set_title('TPS - Sagittal')\n",
    "        axs[1, 0].axis('off')\n",
    "        \n",
    "        axs[1, 1].imshow(tps_image[:, Y_c, :].T, cmap='CMRmap_r')\n",
    "        axs[1, 1].set_title('TPS - Coronal')\n",
    "        axs[1, 1].axis('off')\n",
    "        \n",
    "        axs[1, 2].imshow(tps_image[:, :, Z_c].T, cmap='CMRmap_r')\n",
    "        axs[1, 2].set_title('TPS - Axial')\n",
    "        axs[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate scaling factor\n",
    "        scaler = np.max(tps_image) / np.max(rec_image_oriented)\n",
    "        print(f\"\\nScaling factor (TPS/EPID): {scaler:.2f}\")\n",
    "        print(f\"TPS center value: {tps_image[tps_image.shape[0]//2, tps_image.shape[1]//2, tps_image.shape[2]//2]}\")\n",
    "        print(f\"EPID center value: {rec_image_oriented[50, 50, 50]:.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"TPS file not found at: {_TPS_pth}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading TPS data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZATION PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total reconstruction time: {reconstruction_time:.2f} seconds\")\n",
    "print(f\"Number of projections processed: {len(sorted_angles)}\")\n",
    "print(f\"Time per projection: {reconstruction_time/len(sorted_angles):.3f} seconds\")\n",
    "print(f\"CPU cores used: {n_jobs}\")\n",
    "print(f\"Reconstructed volume size: {rec_image.shape}\")\n",
    "print(f\"Total voxels: {np.prod(rec_image.shape):,}\")\n",
    "print(\"\\nOptimizations applied:\")\n",
    "print(\"✓ Parallel DICOM loading\")\n",
    "print(\"✓ Vectorized mathematical operations\")\n",
    "print(\"✓ FFT-based filtering\")\n",
    "print(\"✓ Parallel backprojection processing\")\n",
    "print(\"✓ Memory-efficient data handling\")\n",
    "print(\"✓ Progress tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the reconstructed volume as DICOM\n",
    "# try:\n",
    "#     if 'tps_dcm' in locals():\n",
    "#         # Scale and prepare for DICOM export\n",
    "#         scaled_image = np.int32(rec_image_oriented * scaler)\n",
    "#         \n",
    "#         # Create new DICOM based on TPS template\n",
    "#         write_dicom = tps_dcm.copy()\n",
    "#         write_dicom.NumberOfFrames = str(rec_image.shape[2])\n",
    "#         write_dicom.Rows = rec_image.shape[0]\n",
    "#         write_dicom.Columns = rec_image.shape[1]\n",
    "#         write_dicom.PixelData = scaled_image.tobytes()\n",
    "#         \n",
    "#         # Save with timestamp\n",
    "#         import datetime\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         output_path = f\"E:\\\\CMC\\\\pyprojects\\\\radio_therapy\\\\dose-3d\\\\dataset\\\\3DDose\\\\EPID_OPTIMIZED_{timestamp}.dcm\"\n",
    "#         dcmwrite(output_path, write_dicom)\n",
    "#         print(f\"\\nOptimized reconstruction saved to: {output_path}\")\n",
    "#     else:\n",
    "#         print(\"TPS data not available - cannot save DICOM\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error saving DICOM: {e}\")\n",
    "\n",
    "print(\"\\nOptimized reconstruction complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
